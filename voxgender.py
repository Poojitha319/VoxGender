# -*- coding: utf-8 -*-
"""VoxGender.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1jCNnj1oIE4DaxF2AyNSEeh7TrHUY5shJ

# Voice Gender Identification
![](https://camo.githubusercontent.com/71343726533e8fa852e0b844b068eafbd4d527931cafb0f6ca65e46774d5062b/68747470733a2f2f6861636b65726e6f6f6e2e636f6d2f686e2d696d616765732f312a43686f63485f655578696c35656165584973643372772e706e67)

### Importing Required Libraries and Dataset
"""

# Commented out IPython magic to ensure Python compatibility.
# Ignore  the warnings
import warnings
warnings.filterwarnings('always')
warnings.filterwarnings('ignore')

# data visualisation and manipulation
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from matplotlib import style
import seaborn as sns
import missingno as msno

#configure
# sets matplotlib to inline and displays graphs below the corressponding cell.
# %matplotlib inline
style.use('fivethirtyeight')
sns.set(style='whitegrid',color_codes=True)

#import the necessary modelling algos.
from sklearn.linear_model import LogisticRegression
from sklearn.svm import LinearSVC
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import GradientBoostingClassifier
from sklearn import datasets
from sklearn.naive_bayes import GaussianNB

#model selection
from sklearn.model_selection import train_test_split
from sklearn.model_selection import KFold
from sklearn.metrics import accuracy_score,precision_score,recall_score,confusion_matrix,roc_curve,roc_auc_score
from sklearn.model_selection import GridSearchCV

#preprocess.
from sklearn.preprocessing import MinMaxScaler,StandardScaler,LabelEncoder,OneHotEncoder
# Import SimpleImputer from sklearn.impute instead of Imputer from sklearn.preprocessing
from sklearn.impute import SimpleImputer

"""### Loading the Dataset"""

train=pd.read_csv(r'/content/voice.csv')

train.head(10)

"""<a id="content2"></a>
*****************************
## Exploratory Data Analysis (EDA)

### The Features and the 'Target' variable
"""

df=train.copy()

df.head(10)

df.shape

df.index

df.columns # give a short description of each feature.

"""**#A short description as on 'Data' tab on kaggle is :**

####

**meanfreq**: mean frequency (in kHz)

**sd**: standard deviation of frequency

**median**: median frequency (in kHz)

**Q25**: first quantile (in kHz)

**Q75**: third quantile (in kHz)

**IQR**: interquantile range (in kHz)

**skew**: skewness (see note in specprop description)

**kurt**: kurtosis (see note in specprop description)

**sp.ent**: spectral entropy

**sfm**: spectral flatness

**mode**: mode frequency

**centroid**: frequency centroid (see specprop)

**peakf**: peak frequency (frequency with highest energy)

**meanfun**: average of fundamental frequency measured across acoustic signal

**minfun**: minimum fundamental frequency measured across acoustic signal

**maxfun**: maximum fundamental frequency measured across acoustic signal

**meandom**: average of dominant frequency measured across acoustic signal

**mindom**: minimum of dominant frequency measured across acoustic signal

**maxdom**: maximum of dominant frequency measured across acoustic signal

**dfrange**: range of dominant frequency measured across acoustic signal

**modindx**: modulation index. Calculated as the accumulated absolute difference between adjacent measurements of fundamental frequencies divided by the frequency range

**label**: male or female

#### Point to be Noted
Note that we have 3168 voice samples  and for each of sample 20 different acoustic properties are recorded. Finally the 'label' column is the target variable which we have to predict which is the gender of the person.

### Missing Values Treatment
"""

# check for null values.
df.isnull().any()

msno.matrix(df)  # just to visualize. no missing value.

"""### Univariate Analysis

In this section I have performed the univariate analysis. Note that since all of the features are 'numeric' the most reasonable way to plot them would either be a 'histogram' or a 'boxplot'.

Also note that univariate analysis is useful for outlier detection. Hence besides plotting a boxplot and a histogram for each column or feature, I have written a small utility function which tells the remaining no of observations for each feature if  we remove its outliers.

#### To detect the outliers I have used the standard 1.5 InterQuartileRange (IQR) rule which states that any observation lesser than  'first quartile - 1.5 IQR' or greater than 'third quartile +1.5 IQR' is an outlier.
"""

df.describe()

def calc_limits(feature):
    q1,q3=df[feature].quantile([0.25,0.75])
    iqr=q3-q1
    rang=1.5*iqr
    return(q1-rang,q3+rang)

def plot(feature):
    fig,axes=plt.subplots(1,2)
    sns.boxplot(data=df,x=feature,ax=axes[0])
    sns.distplot(a=df[feature],ax=axes[1],color='#ff4125')
    fig.set_size_inches(15,5)

    lower,upper = calc_limits(feature)
    l=[df[feature] for i in df[feature] if i>lower and i<upper]
    print("Number of data points remaining if outliers removed : ",len(l))

plot('meanfreq')

"""#### INFERENCES FROM THE PLOT--

1) First of all note that the values are in compliance with that observed from describe method data frame..

2) Note that we have a couple of outliers w.r.t. to 1.5 quartile rule (reprsented by a 'dot' in the box plot).Removing  these data points or outliers leaves us with around 3104 values.

3) Also note from the distplot that the distribution seems to be a bit -ve skewed hence we can normalize to make the distribution a bit more symmetric.

4) LASTLY NOTE THAT A LEFT TAIL DISTRIBUTION HAS MORE OUTLIERS ON THE SIDE BELOW TO Q1 AS EXPECTED AND A RIGHT TAIL HAS ABOVE THE Q3.

#### Similar other plots can be inferenced.
"""

plot('sd')

plot('median')

plot('Q25')

plot('IQR')

plot('skew')

plot('kurt')

plot('sp.ent')

plot('sfm')

plot('meanfun')

sns.countplot(data=df,x='label')

df['label'].value_counts()

"""####  Note that  we have equal no of observations for the 'males' and the 'females'. Hence it is a balanced class problem.

### Bivariate Analysis

**Corealtion b/w Features**

In this section I have analyzed the corelation between different features. To do it I have plotted a 'heat map' which clearly visulizes the corelation between different features.
"""

temp = []
for i in df.label:
    if i == 'male':
        temp.append(1)
    else:
        temp.append(0)
df['label'] = temp

#corelation matrix.
cor_mat= df[:].corr()
mask = np.array(cor_mat)
mask[np.tril_indices_from(mask)] = False
fig=plt.gcf()
fig.set_size_inches(30,12)
sns.heatmap(data=cor_mat,mask=mask,square=True,annot=True,cbar=True)

"""#### SOME INFERENCES FROM THE ABOVE HEATMAP--

1) Mean frequency is  moderately related to label.

2) IQR and label tend to have a strong positive corelation.

3) Spectral entropy is also quite highly corelated with the label while sfm is moderately related with label.

4) skewness and kurtosis aren't much related with label.

5) meanfun is highly negatively corelated with  the label.

6) Centroid and median have a high positive corelationas expected from their formulae.

7) ALSO NOTE THAT MEANFREQ AND CENTROID ARE EXACTLY SAME FEATURES AS PER FORMULAE AND VALUES ALSO. HENCE THEIR CORELATION IS PERFCET 1. IN THAT CASE WE CAN DROP ANY COLUMN. note that centroid in general has a high degree of corelation with most of the other features.

SO I WILL DROP THE 'CENTROID' COLUMN.

8) sd is highly positively related to sfm and so is sp.ent to sd.

9) kurt and skew are also highly corelated.

10) meanfreq is highly related to medaina s well as Q25.

11) IQR is highly corelated to sd.

12) Finally self relation ie of a feature to itself is equal to 1 as expected.

#### Note that we can drop some highly corelated features as they add redundancy to the model  but  let us keep all the features for now. In case of highly corelated features we can use dimensionality reduction techniques like Principal Component Analysis(PCA) to reduce our feature space.
"""

df.drop('centroid',axis=1,inplace=True)

"""**Plotting the Features against the 'Target' variable**

Here I have just written a small utility function that plots the 'label' column vs the provided feature on a boxplot. In this way I have plotted some of the features against our target variable. This makes it easier to see the effect of the corressponding feature on the 'label'.
"""

# drawing features against the target variable.

def plot_against_target(feature):
    # sns.factorplot(data=df,y=feature,x='label',kind='box') # Replaced factorplot with catplot
    sns.catplot(data=df,y=feature,x='label',kind='box')
    fig=plt.gcf()
    fig.set_size_inches(7,7)

plot_against_target('meanfreq') # 0 for females and 1 for males.

"""#### INFERENCES--

1) Firstly note that 0->'female' and 1->'male'.

2) Note that the boxpot depicts that the females in genral have higher mean frequencies than their male counterparts and which is a generally accepted fact.

#### Again similar inferences can be drawn.
"""

plot_against_target('sd')



plot_against_target('median')

plot_against_target('Q25')

plot_against_target('IQR')

"""#### Note here that  there is a remarkable difference b/w the inter quartile ranges of males and females.This is evident from the strong relation between 'label' and the 'IQR' in the heatmap plotted above."""

plot_against_target('sp.ent')

plot_against_target('sfm')

plot_against_target('meanfun')

"""#### Again high difference in females and males mean fundamental frequency. This is evident from the heat map which clearly shows the high corelation between meanfun and the 'label'.

#### Now we move onto analyzing different features pairwise. Since all the features are continuous the most reasonable way to do this is plotting the scatter plots for each feature pair. I have also distinguished males and feamles on the same plot which makes it a bit easier to compare the variation of features within the two classes.
"""

g = sns.PairGrid(df[['meanfreq','sd','median','Q25','IQR','sp.ent','sfm','meanfun','label']], hue = "label")
g = g.map(plt.scatter).add_legend()

"""<a id="content3"></a>
************************
###  Outlier Treatment

In this section I have dealt with the outliers.  Note that we discovered the potential outliers in the **'univariate analysis' ** section. Now to remove those outliers we can either remove the corressponding data points or impute them with some other statistical quantity like median (robust to outliers) etc..

#### For now I shall be removing all the observations or data points which are outlier to 'any' feature. Note that this substantially reduces the dataset size.
"""

# removal of any data point which is an outlier for any fetaure.
for col in df.columns:
    lower,upper=calc_limits(col)
    df = df[(df[col] >lower) & (df[col]<upper)]

df.shape

df.head(10)

"""<a id="content4"></a>
****************
### Feature Engineering.

**Dropping the features**

I have dropped some columns which according to my analysis proved to be less useful or redundant.
"""

temp_df=df.copy()

temp_df.drop(['skew','kurt','mindom','maxdom'],axis=1,inplace=True) # only one of maxdom and dfrange.
temp_df.head(10)
#df.head(10)

"""**Creating new features**

I have done two new things. Firstly I have made 'meanfreq','median' and 'mode' to comply by the standard relation->

####     ......................................................................................3*Median=2*Mean +Mode.........................................................................

####  For this I have adjusted values in the 'median' column as shown below. You can alter values in any of the other column say the 'meanfreq' column.
"""

temp_df['meanfreq']=temp_df['meanfreq'].apply(lambda x:x*2)
temp_df['median']=temp_df['meanfreq']+temp_df['mode']
temp_df['median']=temp_df['median'].apply(lambda x:x/3)

temp_df.head(10)

sns.boxplot(data=temp_df,y='median',x='label') # seeing the new 'median' against the 'label'.

"""The second new feature that I have added is  a new feature to mesure the 'skewness'.

#### For this I have used the 'Karl Pearson Coefficent' which is calculated as shown below->

**..........................................................Coefficent = (Mean - Mode )/StandardDeviation......................................................**

**You can also try some other coefficient also and see how it comapres with the target i.e. the 'label' column.**
"""

temp_df['pear_skew']=temp_df['meanfreq']-temp_df['mode']
temp_df['pear_skew']=temp_df['pear_skew']/temp_df['sd']
temp_df.head(10)

sns.boxplot(data=temp_df,y='pear_skew',x='label') # plotting new 'skewness' against the 'label'.

"""<a id="content5"></a>
***************
### Preparing the Data

**Normalizing the Features.**
"""

scaler=StandardScaler()
scaled_df=scaler.fit_transform(temp_df.drop('label',axis=1))
X=scaled_df
Y=df['label'].to_numpy()

"""**Splitting into Training and Validation sets.**"""

x_train,x_test,y_train,y_test=train_test_split(X,Y,test_size=0.20,random_state=42)

"""<a id="content6"></a>
************************
## Classification Models

**The following models that we are going to use -**
  * **Logistic Regression** : Logistic regression is a statistical model that in its basic form uses a logistic function to model a binary dependent variable, although many more complex extensions exist. In regression analysis, logistic regression (or logit regression) is estimating the parameters of a logistic model (a form of binary regression).
  
  
  * **Decision Tree Classifier** : Decision Tree is a Supervised learning technique that can be used for both classification and Regression problems, but mostly it is preferred for solving Classification problems. It is a tree-structured classifier, where internal nodes represent the features of a dataset, branches represent the decision rules and each leaf node represents the outcome.
  
  
  * **Random Forest Classifier** : Random Forest is a popular machine learning algorithm that belongs to the supervised learning technique. It can be used for both Classification and Regression problems in ML. It is based on the concept of ensemble learning, which is a process of combining multiple classifiers to solve a complex problem and to improve the performance of the model.
  
  
  * **Gradient Boosting** : This method is expected to be called several times consecutively on different chunks of a dataset so as to implement out-of-core or online learning. This is especially useful when the whole dataset is too big to fit in memory at once. This method has some performance and numerical stability overhead, hence it is better to call partial_fit on chunks of data that are as large as possible (as long as fitting in the memory budget) to hide the overhead.
  
  
  * **KNN algorithm** : K-Nearest Neighbour is one of the simplest Machine Learning algorithms based on Supervised Learning technique. K-NN algorithm assumes the similarity between the new case/data and available cases and put the new case into the category that is most similar to the available categories. K-NN algorithm stores all the available data and classifies a new data point based on the similarity. This means when new data appears then it can be easily classified into a well suite category by using K- NN algorithm.
  
  
  * **Support Vector Machine Algorithm** : Support Vector Machine or SVM is one of the most popular Supervised Learning algorithms, which is used for Classification as well as Regression problems. However, primarily, it is used for Classification problems in Machine Learning. The goal of the SVM algorithm is to create the best line or decision boundary that can segregate n-dimensional space into classes so that we can easily put the new data point in the correct category in the future. This best decision boundary is called a hyperplane.


We are going to use these six algorithms and based on the scores of the models the most fitted algorithm will be set! Now let's check out the algorithms.

### Logistic Regression

Logistic regression is a statistical model that in its basic form uses a logistic function to model a binary dependent variable, although many more complex extensions exist. In regression analysis, logistic regression (or logit regression) is estimating the parameters of a logistic model (a form of binary regression).
"""

clf_lr=LogisticRegression()
clf_lr.fit(x_train,y_train)
pred=clf_lr.predict(x_test)
print(accuracy_score(pred,y_test))

"""### K-Nearest Neighbours Algorithm

K-Nearest Neighbour is one of the simplest Machine Learning algorithms based on Supervised Learning technique. K-NN algorithm assumes the similarity between the new case/data and available cases and put the new case into the category that is most similar to the available categories. K-NN algorithm stores all the available data and classifies a new data point based on the similarity. This means when new data appears then it can be easily classified into a well suite category by using K- NN algorithm.
"""

clf_knn=KNeighborsClassifier()
clf_knn.fit(x_train,y_train)
pred=clf_knn.predict(x_test)
print(accuracy_score(pred,y_test))

"""### Support Vector Machine Algorithm

Support Vector Machine or SVM is one of the most popular Supervised Learning algorithms, which is used for Classification as well as Regression problems. However, primarily, it is used for Classification problems in Machine Learning. The goal of the SVM algorithm is to create the best line or decision boundary that can segregate n-dimensional space into classes so that we can easily put the new data point in the correct category in the future. This best decision boundary is called a hyperplane.
"""

clf_svm=SVC()
clf_svm.fit(x_train,y_train)
pred=clf_svm.predict(x_test)
print(accuracy_score(pred,y_test))

import pickle

# Save the model
with open('svm_model.pkl', 'wb') as file:
    pickle.dump(clf_svm, file)

# Later, to load the model:
# with open('svm_model.pkl', 'rb') as file:
#     clf_svm = pickle.load(file)

"""### Decision Tree Classifier Algorithm

Decision Tree is a Supervised learning technique that can be used for both classification and Regression problems, but mostly it is preferred for solving Classification problems. It is a tree-structured classifier, where internal nodes represent the features of a dataset, branches represent the decision rules and each leaf node represents the outcome.
"""

clf_dt=DecisionTreeClassifier()
clf_dt.fit(x_train,y_train)
pred=clf_dt.predict(x_test)
print(accuracy_score(pred,y_test))

import pickle

# Save the model
with open('decision_tree.pkl', 'wb') as file:
    pickle.dump(clf_dt, file)

# Later, to load the model:
# with open('decision_tree.pkl', 'rb') as file:
#     clf_svm = pickle.load(file)

"""### Random Forest Classifier Algorithm

Random Forest is a popular machine learning algorithm that belongs to the supervised learning technique. It can be used for both Classification and Regression problems in ML. It is based on the concept of ensemble learning, which is a process of combining multiple classifiers to solve a complex problem and to improve the performance of the model.
"""

clf_rf=RandomForestClassifier()
clf_rf.fit(x_train,y_train)
pred=clf_rf.predict(x_test)
print(accuracy_score(pred,y_test))

import pickle

# Save the model
with open('random_for.pkl', 'wb') as file:
    pickle.dump(clf_rf, file)

# Later, to load the model:
# with open('random_for.pkl', 'rb') as file:
#     clf_svm = pickle.load(file)

"""### Gradient Boosting

This method is expected to be called several times consecutively on different chunks of a dataset so as to implement out-of-core or online learning. This is especially useful when the whole dataset is too big to fit in memory at once. This method has some performance and numerical stability overhead, hence it is better to call partial_fit on chunks of data that are as large as possible (as long as fitting in the memory budget) to hide the overhead.
"""

clf_gb=GradientBoostingClassifier()
clf_gb.fit(x_train,y_train)
pred=clf_gb.predict(x_test)
print(accuracy_score(pred,y_test))

import pickle

# Save the model
with open('GB.pkl', 'wb') as file:
    pickle.dump(clf_rf, file)

# Later, to load the model:
# with open('GB.pkl', 'rb') as file:
#     clf_svm = pickle.load(file)

"""#### We can now move onto comparing the results of various modelling algorithms. for tthis I shall combine the results of all models in a data frame and then plot using  a barplot ."""

models=[LogisticRegression(),LinearSVC(),SVC(kernel='rbf'),KNeighborsClassifier(),RandomForestClassifier(),
        DecisionTreeClassifier(),GradientBoostingClassifier(),GaussianNB()]
model_names=['LogisticRegression','LinearSVM','rbfSVM','KNearestNeighbors','RandomForestClassifier','DecisionTree',
             'GradientBoostingClassifier','GaussianNB']

acc=[]
d={}

for model in range(len(models)):
    clf=models[model]
    clf.fit(x_train,y_train)
    pred=clf.predict(x_test)
    acc.append(accuracy_score(pred,y_test))

d={'Modelling Algo':model_names,'Accuracy':acc}

acc_frame=pd.DataFrame(d)
acc_frame

sns.barplot(y='Modelling Algo',x='Accuracy',data=acc_frame)

"""<a id="content7"></a>
**********************
### Parameter Tuning with GridSearchCV

I have tuned only SVM Similarly other algorithms can be tuned.
"""

params_dict={'C':[0.001,0.01,0.1,1,10,100],'gamma':[0.001,0.01,0.1,1,10,100],'kernel':['linear','rbf']}
clf=GridSearchCV(estimator=SVC(),param_grid=params_dict,scoring='accuracy',cv=10)
clf.fit(x_train,y_train)

clf.best_score_

clf.best_params_

print(accuracy_score(clf.predict(x_test),y_test))

print(precision_score(clf.predict(x_test),y_test))

"""**The precision is almost 99.5 % which is quite high. After tuning SVM gives an amazing accuracy of around 99.1 %. Similarly tuning other algorithms parameters might give even greater accuracy !!!**

*******************
### Comparative analysis among the algorithms for this project

We have deployed six machine learning algorithms and every algorithm is deployed successfully without any hesitation. We have checked the accuracy of the models based on the accuracy score of each of the models. Now let's take a look at the scores of each models.

|Name of the Model|Accuracy Score|
|:---:|:---:|
|Logistic Regression|98.17|
|Decision Tree Classifier|94.51|
|Random Forest Classifier|97.26|
|Gradient Boosting|97.46|
|KNN Algorithm|98.17|
|Support Vector Machine Algorithm|99.10|
| Support Vector Machine with Grid search on 10-fold CV|99.50|

**********************************

## Conclusion

**Comparing all those scores scored by the machine learning algorithms, it is clear that Support Vector Machine with Grid search on 10-fold CV is having the upper hand in case of this dataset and after this, we can use KNN algorithm, which is also having good score as compared to the other deployed algorithms**

Best Fitted Models ranking -
1. Support Vector Machine with Grid search on 10-fold CV
2. KNN Algorithm
3. Logistic Regression
4. Gradient Boosting
5. Random Forest Classifier
6. Decision Tree Classifier
"""